#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤ RAG-—Å–∏—Å—Ç–µ–º—ã
–í—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã, –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ—É–¥–∞—á –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
"""

import pandas as pd
import json
import re
from collections import defaultdict, Counter
from datetime import datetime
from pathlib import Path
# import matplotlib.pyplot as plt  # –£–±—Ä–∞–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
# import seaborn as sns

class LogAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤ RAG-—Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        self.df = None
        self.load_logs()
    
    def load_logs(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–æ–≤ –∏–∑ CSV"""
        try:
            # –ß–∏—Ç–∞–µ–º CSV, –ø—Ä–æ–ø—É—Å–∫–∞—è –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            self.df = pd.read_csv(self.csv_path)
            
            # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö
            if 'timestamp' not in self.df.columns:
                self.df.columns = [
                    'timestamp', 'query_text', 'chunks_found', 'chunks_used',
                    'response_length', 'response_successful', 'sources',
                    'similarity_scores', 'response_time_ms', 'error_message',
                    'session_id', 'user_feedback'
                ]
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])
            self.df['response_successful'] = self.df['response_successful'].astype(bool)
            
            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.df)} –∑–∞–ø–∏—Å–µ–π –ª–æ–≥–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–≥–æ–≤: {e}")
            self.df = pd.DataFrame()
    
    def analyze_failures(self):
        """–ê–Ω–∞–ª–∏–∑ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        print("\nüîç –ê–ù–ê–õ–ò–ó –ù–ï–£–î–ê–ß–ù–´–• –ó–ê–ü–†–û–°–û–í")
        print("="*50)
        
        if self.df.empty:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_queries = len(self.df)
        failed_queries = len(self.df[~self.df['response_successful']])
        success_rate = (total_queries - failed_queries) / total_queries * 100
        
        print(f"üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_queries}")
        print(f"  –ù–µ—É–¥–∞—á–Ω—ã—Ö: {failed_queries}")
        print(f"  –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%")
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        failed_df = self.df[~self.df['response_successful']]
        
        if not failed_df.empty:
            print(f"\n‚ùå –¢–µ–º—ã —Å —á–∞—Å—Ç—ã–º–∏ –Ω–µ—É–¥–∞—á–∞–º–∏:")
            failure_queries = failed_df['query_text'].value_counts()
            for query, count in failure_queries.head(10).items():
                print(f"  '{query}' - {count} —Ä–∞–∑")
            
            # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ—É–¥–∞—á
            print(f"\nüîç –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:")
            patterns = self._analyze_query_patterns(failed_df['query_text'].tolist())
            for pattern, count in patterns.most_common(5):
                print(f"  {pattern}: {count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        return {
            'total_queries': total_queries,
            'failed_queries': failed_queries,
            'success_rate': success_rate,
            'failure_patterns': failure_queries.to_dict() if not failed_df.empty else {}
        }
    
    def analyze_sources(self):
        """–ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        print("\nüìö –ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í")
        print("="*50)
        
        if self.df.empty:
            return
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        all_sources = []
        for sources_str in self.df['sources']:
            if pd.notna(sources_str) and sources_str != '[]':
                try:
                    sources = json.loads(sources_str.replace("'", '"'))
                    all_sources.extend(sources)
                except:
                    pass
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        source_counter = Counter(all_sources)
        
        print(f"üìä –ù–∞–∏–±–æ–ª–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:")
        for source, count in source_counter.most_common(10):
            print(f"  {source}: {count} —Ä–∞–∑")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        print(f"\nüéØ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
        self._analyze_source_quality()
        
        return source_counter
    
    def analyze_knowledge_gaps(self):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –∑–Ω–∞–Ω–∏—è—Ö"""
        print("\nüï≥Ô∏è  –ê–ù–ê–õ–ò–ó –ü–†–û–ë–ï–õ–û–í –í –ó–ù–ê–ù–ò–Ø–•")
        print("="*50)
        
        # –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        removed_entities_file = Path("db_prepare/removed_entities.txt")
        removed_entities = []
        if removed_entities_file.exists():
            with open(removed_entities_file, 'r', encoding='utf-8') as f:
                removed_entities = [line.strip() for line in f if line.strip()]
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —É–¥–∞–ª–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        removed_queries = []
        for _, row in self.df.iterrows():
            query = row['query_text'].lower()
            for entity in removed_entities:
                if entity.lower() in query:
                    removed_queries.append({
                        'query': row['query_text'],
                        'entity': entity,
                        'successful': row['response_successful'],
                        'chunks_found': row['chunks_found']
                    })
        
        print(f"üéØ –ó–∞–ø—Ä–æ—Å—ã –ø–æ —É–¥–∞–ª–µ–Ω–Ω—ã–º —Å—É—â–Ω–æ—Å—Ç—è–º:")
        for rq in removed_queries:
            status = "‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω" if not rq['successful'] else "‚ùå –ù–µ–≤–µ—Ä–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω"
            print(f"  '{rq['query']}' ({rq['entity']}) - {status}")
        
        # –í—ã—è–≤–ª–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        print(f"\nüîç –î—Ä—É–≥–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã:")
        failed_df = self.df[~self.df['response_successful']]
        unique_failures = failed_df['query_text'].unique()
        
        for query in unique_failures[:5]:
            if not any(entity.lower() in query.lower() for entity in removed_entities):
                print(f"  '{query}' - –≤–æ–∑–º–æ–∂–Ω—ã–π –Ω–æ–≤—ã–π –ø—Ä–æ–±–µ–ª")
        
        return {
            'removed_entities_queries': removed_queries,
            'potential_gaps': unique_failures.tolist()
        }
    
    def _analyze_query_patterns(self, queries):
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö"""
        patterns = Counter()
        
        for query in queries:
            query_lower = query.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞
            if query_lower.startswith(('–∫—Ç–æ —Ç–∞–∫–æ–π', '–∫—Ç–æ —Ç–∞–∫–∞—è')):
                patterns['–í–æ–ø—Ä–æ—Å—ã –æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞—Ö'] += 1
            elif query_lower.startswith(('—á—Ç–æ —Ç–∞–∫–æ–µ', '—á—Ç–æ —ç—Ç–æ')):
                patterns['–í–æ–ø—Ä–æ—Å—ã –æ –ø—Ä–µ–¥–º–µ—Ç–∞—Ö/–ø–æ–Ω—è—Ç–∏—è—Ö'] += 1
            elif '—Ä–∞—Å—Å–∫–∞–∂–∏' in query_lower:
                patterns['–ó–∞–ø—Ä–æ—Å—ã –Ω–∞ —Ä–∞—Å—Å–∫–∞–∑'] += 1
            elif any(word in query_lower for word in ['–∫–∞–∫', '–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º']):
                patterns['–í–æ–ø—Ä–æ—Å—ã –æ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö'] += 1
            else:
                patterns['–î—Ä—É–≥–∏–µ —Ç–∏–ø—ã –≤–æ–ø—Ä–æ—Å–æ–≤'] += 1
        
        return patterns
    
    def _analyze_source_quality(self):
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        successful_queries = self.df[self.df['response_successful']]
        failed_queries = self.df[~self.df['response_successful']]
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = defaultdict(lambda: {'success': 0, 'failed': 0})
        
        for _, row in self.df.iterrows():
            if pd.notna(row['sources']) and row['sources'] != '[]':
                try:
                    sources = json.loads(row['sources'].replace("'", '"'))
                    for source in sources:
                        category = source.split('/')[0] if '/' in source else 'unknown'
                        if row['response_successful']:
                            categories[category]['success'] += 1
                        else:
                            categories[category]['failed'] += 1
                except:
                    pass
        
        print("üìÇ –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for category, stats in categories.items():
            total = stats['success'] + stats['failed']
            if total > 0:
                success_rate = stats['success'] / total * 100
                print(f"  {category}: {success_rate:.1f}% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ ({stats['success']}/{total})")
    
    def generate_recommendations(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ")
        print("="*50)
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        failure_analysis = self.analyze_failures()
        source_analysis = self.analyze_sources()
        gap_analysis = self.analyze_knowledge_gaps()
        
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        if failure_analysis['success_rate'] < 80:
            recommendations.append({
                'priority': '–í–´–°–û–ö–ò–ô',
                'area': '–ü—Ä–æ–º–ø—Ç—ã',
                'issue': f"–ù–∏–∑–∫–∞—è –æ–±—â–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å ({failure_analysis['success_rate']:.1f}%)",
                'action': '–£–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –∏ –ª–æ–≥–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∑–Ω–∞–Ω–∏–π
        removed_queries = gap_analysis['removed_entities_queries']
        incorrect_removed = [rq for rq in removed_queries if rq['successful']]
        
        if incorrect_removed:
            recommendations.append({
                'priority': '–í–´–°–û–ö–ò–ô',
                'area': '–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π',
                'issue': f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π ({len(incorrect_removed)} —Å–ª—É—á–∞–µ–≤)",
                'action': '–î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç—è—Ö –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å –ª–æ–≥–∏–∫—É –æ—Ç–∫–∞–∑–æ–≤'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —á–∞—Å—Ç—ã–º –Ω–µ—É–¥–∞—á–∞–º
        if failure_analysis['failure_patterns']:
            top_failure = max(failure_analysis['failure_patterns'].items(), key=lambda x: x[1])
            if top_failure[1] > 2:
                recommendations.append({
                    'priority': '–°–†–ï–î–ù–ò–ô',
                    'area': '–ö–æ–Ω—Ç–µ–Ω—Ç',
                    'issue': f"–ß–∞—Å—Ç—ã–µ –Ω–µ—É–¥–∞—á–∏ –ø–æ —Ç–µ–º–µ '{top_failure[0]}' ({top_failure[1]} —Ä–∞–∑)",
                    'action': '–†–∞—Å—à–∏—Ä–∏—Ç—å –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ'
                })
        
        # –í—ã–≤–æ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. [{rec['priority']}] {rec['area']}")
            print(f"   –ü—Ä–æ–±–ª–µ–º–∞: {rec['issue']}")
            print(f"   –î–µ–π—Å—Ç–≤–∏–µ: {rec['action']}")
            print()
        
        return recommendations
    
    def create_summary_report(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("\nüìã –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –õ–û–ì–û–í")
        print("="*60)
        
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
        failure_analysis = self.analyze_failures()
        source_analysis = self.analyze_sources()
        gap_analysis = self.analyze_knowledge_gaps()
        recommendations = self.generate_recommendations()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report = {
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_queries': failure_analysis['total_queries'],
                'success_rate': failure_analysis['success_rate'],
                'failed_queries': failure_analysis['failed_queries']
            },
            'failure_analysis': failure_analysis,
            'source_analysis': dict(source_analysis.most_common(20)),
            'knowledge_gaps': gap_analysis,
            'recommendations': recommendations
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        report_file = Path("logs/log_analysis_report.json")
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        
        return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    
    # –ü—É—Ç—å –∫ –ª–æ–≥–∞–º
    log_path = "logs/query_logs/queries.csv"
    
    if not Path(log_path).exists():
        print(f"‚ùå –§–∞–π–ª –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {log_path}")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = LogAnalyzer(log_path)
    
    # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    analyzer.create_summary_report()


if __name__ == "__main__":
    main()
