#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ–∫—Ä—ã—Ç–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø–æ–∫—Ä—ã—Ç–∏–∏ —Ç–µ–º –∏ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã
"""

import json
import sqlite3
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any
import re
from datetime import datetime

class CoverageAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ–∫—Ä—ã—Ç–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, knowledge_base_path="knowledge_base", logs_path="logs/query_logs/queries.db"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            knowledge_base_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
            logs_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ª–æ–≥–æ–≤
        """
        self.knowledge_base_path = Path(knowledge_base_path)
        self.logs_path = Path(logs_path)
        
    def analyze_knowledge_base_structure(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print("üìÅ –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        structure = {
            'categories': {},
            'total_files': 0,
            'total_size_bytes': 0,
            'file_sizes': [],
            'category_distribution': {}
        }
        
        # –û–±—Ö–æ–¥ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        for file_path in self.knowledge_base_path.rglob("*.txt"):
            if file_path.is_file():
                structure['total_files'] += 1
                
                # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                file_size = file_path.stat().st_size
                structure['total_size_bytes'] += file_size
                structure['file_sizes'].append(file_size)
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏—è (–ø–∞–ø–∫–∞)
                category = file_path.parent.name
                if category not in structure['categories']:
                    structure['categories'][category] = {
                        'files': [],
                        'count': 0,
                        'total_size': 0
                    }
                
                structure['categories'][category]['files'].append(file_path.name)
                structure['categories'][category]['count'] += 1
                structure['categories'][category]['total_size'] += file_size
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, data in structure['categories'].items():
            structure['category_distribution'][category] = {
                'count': data['count'],
                'percentage': (data['count'] / structure['total_files']) * 100,
                'avg_size': data['total_size'] / data['count'] if data['count'] > 0 else 0
            }
        
        return structure
    
    def analyze_content_topics(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–º –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
        print("üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–º –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π...")
        
        topics = {
            'entities': defaultdict(int),
            'keywords': defaultdict(int),
            'removed_entities': [],
            'categories_coverage': {},
            'content_stats': {}
        }
        
        all_text = ""
        category_texts = defaultdict(str)
        
        # –°–±–æ—Ä –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
        for file_path in self.knowledge_base_path.rglob("*.txt"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        all_text += content + " "
                        
                        category = file_path.parent.name
                        category_texts[category] += content + " "
                        
                        # –ü–æ–∏—Å–∫ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
                        removed_pattern = r'\[–£–î–ê–õ–ï–ù–û: ([^\]]+)\]'
                        removed_matches = re.findall(removed_pattern, content)
                        topics['removed_entities'].extend(removed_matches)
                        
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words = re.findall(r'\b[–ê-–ØA-Z][–∞-—èa-z]+\b', all_text)
        topics['keywords'] = dict(Counter(words).most_common(50))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, text in category_texts.items():
            word_count = len(text.split())
            char_count = len(text)
            
            topics['categories_coverage'][category] = {
                'word_count': word_count,
                'char_count': char_count,
                'unique_words': len(set(text.lower().split())),
                'removed_entities': len(re.findall(r'\[–£–î–ê–õ–ï–ù–û:', text))
            }
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        topics['content_stats'] = {
            'total_words': len(all_text.split()),
            'total_chars': len(all_text),
            'unique_words': len(set(all_text.lower().split())),
            'removed_entities_count': len(topics['removed_entities']),
            'unique_removed_entities': list(set(topics['removed_entities']))
        }
        
        return topics
    
    def analyze_query_logs(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        print("üìä –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤...")
        
        if not self.logs_path.exists():
            return {'error': '–õ–æ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}
        
        try:
            conn = sqlite3.connect(self.logs_path)
            cursor = conn.cursor()
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            cursor.execute("SELECT COUNT(*) FROM query_logs")
            total_queries = cursor.fetchone()[0]
            
            if total_queries == 0:
                return {'error': '–ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –≤ –ª–æ–≥–∞—Ö'}
            
            # –£—Å–ø–µ—à–Ω—ã–µ vs –Ω–µ—É—Å–ø–µ—à–Ω—ã–µ
            cursor.execute("SELECT response_successful, COUNT(*) FROM query_logs GROUP BY response_successful")
            success_stats = dict(cursor.fetchall())
            
            # –¢–æ–ø –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            cursor.execute("""
                SELECT query_text, COUNT(*) as freq 
                FROM query_logs 
                WHERE response_successful = 0 
                GROUP BY query_text 
                ORDER BY freq DESC 
                LIMIT 10
            """)
            failed_queries = cursor.fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
            cursor.execute("""
                SELECT 
                    CASE 
                        WHEN chunks_found = 0 THEN '0 —á–∞–Ω–∫–æ–≤'
                        WHEN chunks_found <= 2 THEN '1-2 —á–∞–Ω–∫–∞'
                        WHEN chunks_found <= 5 THEN '3-5 —á–∞–Ω–∫–æ–≤'
                        ELSE '5+ —á–∞–Ω–∫–æ–≤'
                    END as chunk_range,
                    COUNT(*) as count
                FROM query_logs 
                GROUP BY chunk_range
            """)
            chunk_distribution = dict(cursor.fetchall())
            
            # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            cursor.execute("""
                SELECT 
                    AVG(response_time_ms) as avg_time,
                    AVG(chunks_found) as avg_chunks,
                    AVG(response_length) as avg_length
                FROM query_logs
            """)
            avg_metrics = cursor.fetchone()
            
            # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            cursor.execute("SELECT sources FROM query_logs WHERE sources != '[]'")
            sources_data = cursor.fetchall()
            
            all_sources = []
            for row in sources_data:
                try:
                    sources = json.loads(row[0])
                    all_sources.extend(sources)
                except:
                    continue
            
            source_usage = dict(Counter(all_sources).most_common(10))
            
            conn.close()
            
            return {
                'total_queries': total_queries,
                'success_stats': success_stats,
                'success_rate': (success_stats.get(1, 0) / total_queries) * 100,
                'failed_queries': failed_queries,
                'chunk_distribution': chunk_distribution,
                'avg_metrics': {
                    'response_time_ms': round(avg_metrics[0] or 0, 2),
                    'chunks_found': round(avg_metrics[1] or 0, 2),
                    'response_length': round(avg_metrics[2] or 0, 2)
                },
                'source_usage': source_usage
            }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤: {e}'}
    
    def identify_knowledge_gaps(self, query_analysis: Dict[str, Any], content_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –∑–Ω–∞–Ω–∏—è—Ö"""
        print("üö® –í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –∑–Ω–∞–Ω–∏—è—Ö...")
        
        gaps = {
            'content_gaps': [],
            'query_gaps': [],
            'coverage_issues': [],
            'recommendations': []
        }
        
        # –ü—Ä–æ–±–µ–ª—ã –∏–∑ —É–¥–∞–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if content_analysis.get('content_stats', {}).get('unique_removed_entities'):
            gaps['content_gaps'] = content_analysis['content_stats']['unique_removed_entities']
        
        # –ü—Ä–æ–±–µ–ª—ã –∏–∑ –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if query_analysis.get('failed_queries'):
            gaps['query_gaps'] = [query[0] for query in query_analysis['failed_queries'][:5]]
        
        # –ü—Ä–æ–±–ª–µ–º—ã –ø–æ–∫—Ä—ã—Ç–∏—è
        if query_analysis.get('success_rate', 100) < 80:
            gaps['coverage_issues'].append(f"–ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {query_analysis['success_rate']:.1f}%")
        
        if content_analysis.get('content_stats', {}).get('removed_entities_count', 0) > 0:
            gaps['coverage_issues'].append(f"–£–¥–∞–ª–µ–Ω–æ {content_analysis['content_stats']['removed_entities_count']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å—É—â–Ω–æ—Å—Ç–µ–π")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        gaps['recommendations'] = self._generate_recommendations(gaps, content_analysis, query_analysis)
        
        return gaps
    
    def _generate_recommendations(self, gaps: Dict, content_analysis: Dict, query_analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
        if gaps['content_gaps']:
            recommendations.append(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—É—â–Ω–æ—Å—Ç—è—Ö: {', '.join(gaps['content_gaps'][:3])}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = content_analysis.get('categories_coverage', {})
        if categories:
            min_category = min(categories.items(), key=lambda x: x[1]['word_count'])
            if min_category[1]['word_count'] < 1000:
                recommendations.append(f"–†–∞—Å—à–∏—Ä–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é '{min_category[0]}' (—Ç–æ–ª—å–∫–æ {min_category[1]['word_count']} —Å–ª–æ–≤)")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
        if query_analysis.get('success_rate', 100) < 90:
            recommendations.append("–£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ - –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        if gaps['query_gaps']:
            recommendations.append(f"–î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —á–∞—Å—Ç—ã—Ö –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if not recommendations:
            recommendations.append("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π")
        
        return recommendations
    
    def generate_coverage_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –ø–æ–∫—Ä—ã—Ç–∏–∏"""
        print("\nüìà –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–ï–¢–ê –û –ü–û–ö–†–´–¢–ò–ò –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô")
        print("="*60)
        
        # –ê–Ω–∞–ª–∏–∑—ã
        structure = self.analyze_knowledge_base_structure()
        content = self.analyze_content_topics()
        queries = self.analyze_query_logs()
        gaps = self.identify_knowledge_gaps(queries, content)
        
        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report = {
            'timestamp': datetime.now().isoformat(),
            'structure_analysis': structure,
            'content_analysis': content,
            'query_analysis': queries,
            'knowledge_gaps': gaps,
            'summary': {
                'total_files': structure['total_files'],
                'total_categories': len(structure['categories']),
                'removed_entities': len(content['content_stats']['unique_removed_entities']),
                'total_queries': queries.get('total_queries', 0),
                'success_rate': queries.get('success_rate', 0),
                'main_gaps': gaps['content_gaps'][:3],
                'top_recommendations': gaps['recommendations'][:3]
            }
        }
        
        return report
    
    def print_report(self, report: Dict[str, Any]):
        """–ü–µ—á–∞—Ç—å –æ—Ç—á–µ—Ç–∞ –≤ —á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ"""
        print("\nüìä –û–¢–ß–ï–¢ –û –ü–û–ö–†–´–¢–ò–ò –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô")
        print("="*60)
        
        # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        summary = report['summary']
        print(f"üìÅ –§–∞–π–ª–æ–≤ –≤ –±–∞–∑–µ: {summary['total_files']}")
        print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {summary['total_categories']}")
        print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π: {summary['removed_entities']}")
        print(f"‚ùì –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {summary['total_queries']}")
        print(f"‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {summary['success_rate']:.1f}%")
        print()
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞
        print("üìÅ –°–¢–†–£–ö–¢–£–†–ê –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:")
        for category, data in report['structure_analysis']['category_distribution'].items():
            print(f"   {category}: {data['count']} —Ñ–∞–π–ª–æ–≤ ({data['percentage']:.1f}%)")
        print()
        
        # –£–¥–∞–ª–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        if summary['main_gaps']:
            print("üö® –û–°–ù–û–í–ù–´–ï –ü–†–û–ë–ï–õ–´ (—É–¥–∞–ª–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏):")
            for gap in summary['main_gaps']:
                print(f"   - {gap}")
            print()
        
        # –ù–µ—É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if report['query_analysis'].get('failed_queries'):
            print("‚ùå –ß–ê–°–¢–´–ï –ù–ï–£–°–ü–ï–®–ù–´–ï –ó–ê–ü–†–û–°–´:")
            for query, count in report['query_analysis']['failed_queries'][:5]:
                print(f"   - {query} ({count} —Ä–∞–∑)")
            print()
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
        for i, rec in enumerate(summary['top_recommendations'], 1):
            print(f"   {i}. {rec}")
        print()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if report['query_analysis'].get('source_usage'):
            print("üìö –°–ê–ú–´–ï –ò–°–ü–û–õ–¨–ó–£–ï–ú–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò:")
            for source, count in list(report['query_analysis']['source_usage'].items())[:5]:
                print(f"   - {source}: {count} —Ä–∞–∑")
        
        print("\n‚úÖ –û—Ç—á–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    def save_report(self, report: Dict[str, Any], filename: str = "coverage_report.json"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    analyzer = CoverageAnalyzer()
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = analyzer.generate_coverage_report()
    
    # –ü–µ—á–∞—Ç—å –æ—Ç—á–µ—Ç–∞
    analyzer.print_report(report)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    analyzer.save_report(report)

if __name__ == "__main__":
    main()
