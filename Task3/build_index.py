#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
"""

import json
import os
import time
from pathlib import Path
from typing import List

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

class KnowledgeBaseIndexer:
    def __init__(self, knowledge_base_path: str = "../Task2/knowledge_base"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–µ—Ä–∞
        
        Args:
            knowledge_base_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        """
        self.knowledge_base_path = Path(knowledge_base_path)
        self.embeddings_model = "all-mpnet-base-v2"
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.embeddings_model,
            model_kwargs={'device': 'cpu'},  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,  # ~2000 —Å–∏–º–≤–æ–ª–æ–≤ (–±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ —á–∞–Ω–∫–∏)
            chunk_overlap=200,  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
        )
        
        self.documents = []
        self.chunks = []
        
    def load_documents(self) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        
        for file_path in self.knowledge_base_path.rglob("*.txt"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    
                    if content:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
                        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                        relative_path = file_path.relative_to(self.knowledge_base_path)
                        category = relative_path.parts[0] if len(relative_path.parts) > 1 else "root"
                        
                        metadata = {
                            "source": str(relative_path),
                            "category": category,
                            "filename": file_path.name,
                            "file_path": str(file_path),
                            "file_size": len(content)
                        }
                        
                        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç LangChain
                        doc = Document(
                            page_content=content,
                            metadata=metadata
                        )
                        self.documents.append(doc)
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return self.documents
    
    def split_documents(self) -> List[Document]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏"""
        print("‚úÇÔ∏è –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
        
        for doc in self.documents:
            chunks = self.text_splitter.split_documents([doc])
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏
            filtered_chunks = []
            for i, chunk in enumerate(chunks):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫–∏ –º–µ–Ω—å—à–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                if len(chunk.page_content.strip()) >= 100:
                    chunk.metadata.update({
                        "chunk_id": f"{doc.metadata['filename']}_{i}",
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    })
                    filtered_chunks.append(chunk)
            
            self.chunks.extend(filtered_chunks)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.chunks)} —á–∞–Ω–∫–æ–≤ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)")
        return self.chunks
    
    def create_vector_index(self, persist_directory: str = "./chroma_db") -> Chroma:
        """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –≤ ChromaDB"""
        print("üîç –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è ChromaDB
        os.makedirs(persist_directory, exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞ –¥–∏—Å–∫ (chromadb>=1.0.* —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        vectorstore = Chroma.from_documents(
            documents=self.chunks,
            embedding=self.embeddings,
            persist_directory=persist_directory
        )
        
        print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {persist_directory}")
        return vectorstore
    
    def test_search(self, vectorstore: Chroma, test_queries: List[str] = None):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        if test_queries is None:
            test_queries = [
                "–ö—Ç–æ —Ç–∞–∫–æ–π Xarn Velgor?",
                "–ß—Ç–æ —Ç–∞–∫–æ–µ Synth Flux?",
                "–†–∞—Å—Å–∫–∞–∂–∏ –æ –±–∏—Ç–≤–µ –Ω–∞ Krael",
                "–ö–∞–∫–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç Wardens?",
                "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è Aurelia Prime?"
            ]
        
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞:")
        print("=" * 50)
        
        for query in test_queries:
            print(f"\nüîç –ó–∞–ø—Ä–æ—Å: {query}")
            results = vectorstore.similarity_search(query, k=3)
            
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result.metadata['source']}")
                print(f"     {result.page_content[:150]}...")
                print()
    
    def save_statistics(self, vectorstore: Chroma, processing_time: float):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        stats = {
            "model": {
                "name": self.embeddings_model,
                "repository": "https://huggingface.co/sentence-transformers/all-mpnet-base-v2",
                "embedding_size": 768
            },
            "knowledge_base": {
                "path": str(self.knowledge_base_path),
                "total_documents": len(self.documents),
                "total_chunks": len(self.chunks)
            },
            "processing": {
                "time_seconds": processing_time,
                "time_minutes": processing_time / 60,
                "chunks_per_second": len(self.chunks) / processing_time
            },
            "vectorstore": {
                "type": "ChromaDB",
                "persist_directory": "./chroma_db"
            }
        }
        
        with open("indexing_stats.json", "w", encoding="utf-8") as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ indexing_stats.json")
        return stats

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞")
    print("=" * 50)
    
    start_time = time.time()
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å–µ—Ä
    indexer = KnowledgeBaseIndexer()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    indexer.load_documents()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    indexer.split_documents()
    
    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    vectorstore = indexer.create_vector_index()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    indexer.test_search(vectorstore)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    processing_time = time.time() - start_time
    stats = indexer.save_statistics(vectorstore, processing_time)
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n" + "=" * 50)
    print("üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –ú–æ–¥–µ–ª—å: {stats['model']['name']}")
    print(f"   –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {stats['model']['embedding_size']}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['knowledge_base']['total_documents']}")
    print(f"   –ß–∞–Ω–∫–æ–≤: {stats['knowledge_base']['total_chunks']}")
    print(f"   –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stats['processing']['time_minutes']:.2f} –º–∏–Ω—É—Ç")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {stats['processing']['chunks_per_second']:.1f} —á–∞–Ω–∫–æ–≤/—Å–µ–∫")
    print("=" * 50)

if __name__ == "__main__":
    main()
