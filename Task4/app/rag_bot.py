"""
RAG-–±–æ—Ç —Å —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Few-shot –∏ Chain-of-Thought –ø–æ–¥—Ö–æ–¥—ã
"""

import os
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RAGConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG-–±–æ—Ç–∞"""
    vector_db_path: str = "chroma_db"
    embedding_model: str = "all-mpnet-base-v2"
    max_results: int = 5
    temperature: float = 0.7
    chunk_size: int = 2000
    chunk_overlap: int = 200


class RAGBot:
    """RAG-–±–æ—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.embeddings = HuggingFaceEmbeddings(
            model_name=config.embedding_model,
            model_kwargs={'device': 'cpu'}
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        persist_dir = os.path.abspath(config.vector_db_path)
        if not os.path.exists(persist_dir):
            raise FileNotFoundError(
                f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {persist_dir}. "
                f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∫–∞—Ç–∞–ª–æ–≥ —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (docker-compose) –∏ –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω (Task3)."
            )
        self.vector_db = Chroma(
            persist_directory=persist_dir,
            embedding_function=self.embeddings
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        self.llm = self._initialize_llm()
        
        # –ü—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫
        self.prompts = self._create_prompts()
        
        logger.info("RAG-–±–æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _create_prompts(self) -> Dict[str, PromptTemplate]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫"""
        
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        base_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å:

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""
        )
        
        # Few-shot –ø—Ä–æ–º–ø—Ç
        few_shot_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –≤—Å–µ–ª–µ–Ω–Ω–æ–π –Ω–∞—É—á–Ω–æ–π —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∏. –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤:

–ü—Ä–∏–º–µ—Ä 1:
–í–æ–ø—Ä–æ—Å: –ö—Ç–æ —Ç–∞–∫–æ–π Xarn Velgor?
–û—Ç–≤–µ—Ç: Xarn Velgor - —ç—Ç–æ –º–æ–≥—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–º–Ω—ã–π –ª–æ—Ä–¥, –±—ã–≤—à–∏–π —É—á–µ–Ω–∏–∫ Wardens, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ—à–µ–ª –Ω–∞ —Ç–µ–º–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É Synth Flux. –û–Ω –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–∏–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ Synth Flux –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Lumen Blade.

–ü—Ä–∏–º–µ—Ä 2:
–í–æ–ø—Ä–æ—Å: –ß—Ç–æ —Ç–∞–∫–æ–µ Synth Flux?
–û—Ç–≤–µ—Ç: Synth Flux - —ç—Ç–æ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–ª–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–∫—Ä—É–∂–∞–µ—Ç –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç –≤—Å–µ –≤ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–∏. –≠—Ç–æ –∏—Å—Ç–æ—á–Ω–∏–∫ —Å–∏–ª—ã –¥–ª—è Wardens –∏ Vyrn, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.

–¢–µ–ø–µ—Ä—å –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""
        )
        
        # Chain-of-Thought –ø—Ä–æ–º–ø—Ç
        cot_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –≤—Å–µ–ª–µ–Ω–Ω–æ–π –Ω–∞—É—á–Ω–æ–π —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Ä–∞—Å—Å—É–∂–¥–∞—è –ø–æ—à–∞–≥–æ–≤–æ.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–î–∞–≤–∞–π —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –ø–æ—à–∞–≥–æ–≤–æ:

1. –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–º, –æ —á–µ–º —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
2. –ù–∞–π–¥–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
3. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
4. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ–º —á–µ—Ç–∫–∏–π –∏ –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç

–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ:"""
        )
        
        return {
            "base": base_prompt,
            "few_shot": few_shot_prompt,
            "chain_of_thought": cot_prompt
        }
    
    def _initialize_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        return ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=self.config.temperature,
            api_key=api_key
        )
    
    def search_documents(self, query: str) -> List[Document]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            results = self.vector_db.similarity_search(
                query, 
                k=self.config.max_results
            )
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []
    
    def format_context(self, documents: List[Document]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not documents:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
            category = doc.metadata.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è')
            chunk_id = doc.metadata.get('chunk_id', 'N/A')
            
            context_parts.append(f"""
--- –î–æ–∫—É–º–µ–Ω—Ç {i} ---
–ò—Å—Ç–æ—á–Ω–∏–∫: {source}
–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}
–ß–∞–Ω–∫: {chunk_id}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:
{doc.page_content}
""")
        
        return "\n".join(context_parts)
    
    def generate_response(self, query: str, technique: str = "base") -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞"""
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = self.search_documents(query)
        context = self.format_context(documents)
        
        # –í—ã–±–æ—Ä –ø—Ä–æ–º–ø—Ç–∞
        if technique not in self.prompts:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ {technique}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é")
            technique = "base"
        
        prompt = self.prompts[technique]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ RAG
        chain = (
            {"context": lambda x: x["context"], "question": lambda x: x["question"]}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM
        response = chain.invoke({
            "context": context,
            "question": query
        })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        formatted_prompt = self.prompts[technique].format(context=context, question=query)

        return {
            "query": query,
            "technique": technique,
            "response": response,
            "context": context,
            "prompt": formatted_prompt,
            "sources": [
                {
                    "source": doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫'),
                    "category": doc.metadata.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è'),
                    "chunk_id": doc.metadata.get('chunk_id', 'N/A'),
                    "content_preview": doc.page_content[:200] + "..."
                }
                for doc in documents
            ],
            "num_sources": len(documents)
        }
    

    

    
    def interactive_mode(self):
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –±–æ—Ç–æ–º"""
        print("ü§ñ RAG-–±–æ—Ç –∑–∞–ø—É—â–µ–Ω –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ")
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞:")
        print("  - base: –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç")
        print("  - few_shot: Few-shot –æ–±—É—á–µ–Ω–∏–µ")
        print("  - chain_of_thought: Chain-of-Thought —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ")
        print("LLM: OpenAI GPT-3.5-turbo")
        print("–í–≤–µ–¥–∏—Ç–µ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
        print("-" * 50)
        
        while True:
            try:
                query = input("\n‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
                
                if query.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥']:
                    print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break
                
                if not query:
                    continue
                
                # –í—ã–±–æ—Ä —Ç–µ—Ö–Ω–∏–∫–∏
                print("\n–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ—Ö–Ω–∏–∫—É –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞:")
                print("1. base (–±–∞–∑–æ–≤–∞—è)")
                print("2. few_shot (few-shot)")
                print("3. chain_of_thought (chain-of-thought)")
                
                choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-3) –∏–ª–∏ Enter –¥–ª—è –±–∞–∑–æ–≤–æ–π: ").strip()
                
                technique_map = {
                    "1": "base",
                    "2": "few_shot", 
                    "3": "chain_of_thought"
                }
                
                technique = technique_map.get(choice, "base")
                
                print(f"\nüîç –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏...")
                result = self.generate_response(query, technique)
                
                print(f"\nü§ñ –û—Ç–≤–µ—Ç ({technique}):")
                print("-" * 40)
                print(result["response"])
                print("-" * 40)
                
                print(f"\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({result['num_sources']}):")
                for i, source in enumerate(result["sources"], 1):
                    print(f"  {i}. {source['source']} ({source['category']})")
                
            except KeyboardInterrupt:
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    from dotenv import load_dotenv
    load_dotenv()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = RAGConfig()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –±–æ—Ç–∞
    bot = RAGBot(config)
    bot.interactive_mode()


if __name__ == "__main__":
    main()
